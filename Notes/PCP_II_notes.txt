** Topics: **

1. Synchronization
2. Barriers
3. Asynchronous Tasks
4. Evaluating Parallel Performance
5. Designing Parallel Programs
6. Challenge Problems

** Tools: **

GCC 7.4.0
make 4.2.1
C++17 Standard
Cygwin on Windows 10
	installed gcc-g++ and make
	add to system path
boost cpp library

******** LECTURE 1 : Synchronization ********

*** Condition Variable ***

Problem:
Busy waiting || Spinning
Repeatedly enquiring lock to check

Solution: Condition Variable
- associated with mutex
- protect section of code with mutual exclusion

With conditional variables thread can place a lock when enetered to the critical section
After the thread exited the critial section releases the lock
Wakes other waiting thread in the waiting room

3 operations:
- wait
- signal
- broadcast

wait:
- Automatically release lock on the mutex
- Go to sleep and enter waiting queue
- Reacquire lock when woken up

signal:
- Wake up one thread from condition variable queue
- Also called 'notify' or 'wake'

broadcast:
- Wake up all threads from condition variable queue
- Also called 'notify' all or 'wake all'

Tip:
By making thread wait in a while loop we can prevent spurious wake up.
In some OSs threads can wake up when not intended to.


while !(SOME_CONDITION)
{
    cv.wait ( lk );
}


*** Producer - Consumer ***

A common design pattern in concurrent programming is
the Producer-Consumer architecture.

Producer thread(s)
- Adds elements to the shared data structure

Consumer thread(s)
- Remove elements from shared data structure

Queue (FIFO)
- first item added is the first item to be consumed

Challenges
- Consider how fast producer produces and consumer consumes

Unbound Queue
- in theory limitless space but limited by memory

Average Rate Of Production < Average Rate Of Consumption

Pipeline
- Chain of processing elements
- Series of producer and consumers are chained

*** Semaphore ***

- Synchronization mechanism to control the access to the critical section
- Can multiple threads utilize this to access to the critical
section at the same time
- Has a counter mechanism, limits the number of threads that can access

Binary semaphore is like mutex
- Mutex can be unlocked by the thread locked it
- Semaphore can be acquired/release by different threads


Producer-Consumer Semaphore
- Two semaphores: fillCount, emptyCount
- Producer:
  - Acquire emptyCount (emptyCount - 1)
  - Put item into buffer
  - Release fillCount (fillCount + 1)
- Consumer:
 - Acquire fillCount (fillCount - 1) 
  - Remove item from buffer
  - Release emptyCount (emptyCount + 1)


******** LECTURE 2 : Barriers ********

*** Race Condition ***

Data Racing - Race Condition are different terms

Data Racing : Two or more threads concurrently access the same memory location
- Solution: Mutual exclusion on the shared resource

Race Condition: Flaw in the timing on the programs execution
- Data races can cause race conditions too (not necessary)

Race conditions are sometimes type of Heisenbug.
Heisenbug: A software bug that disappears when you try to study it.

*** Barriers ***

Synchronize the events to prevent race condition
Has a counter feature

*** Latch ***

One or more threads to wait while other operations are performed
by other one or more threads.

Has a counter feature.

Threads can utilize:
- Wait at a latch till a count value reaches zero
- Or decrement the latch value 

Ways to utilize Latch:
- Initialize count to 1
  - A simple on/off gate
- Initialize count to N
  - Wait for N threads to complete some action
  - Wait for some action to complete N times

.count_down() decrements the count down but does not prevent thread from running
.wait() prevents the thread from running till the counter reaches zero

Barrier vs Latch
- Barrier releases when certain number of threads are waiting
- Latch releases when count value reaches zero

******** LECTURE 3 : Asynchronous Tasks ********

*** Computational Graph ***

'Spawn' = 'Fork'
'Synch' = 'Join'

Work: Time needed to execute all of the tasks in a singel core.
Critical path: Execution time of the longest path [Span]

Ideal Parallelism = work/span

*** Thread Pool ***

Rather than creating new task for every work utilizing the thread pool
can be better solution.
This way overhead of creating new thread is avoided.
Threads get new task from a queue.

*** Future ***

Olaceholder for a result initially unknown
Mechanism to access
Read-only
When result is written 'resolving' 'fulfilling'

*** Divide and Conquer ***

1. Divide the problem into subproblems
2. Conquer the the subproblems by solving the recursively
3. Combine the solutions to the subproblem
 
Common structe is "if" and "else"
Divide until reach 'base case'

Does it really woth it? We need to analyze the method for the problem in hand.

******** LECTURE 4 : Evaluating Parallel Performance ********

*** Speedup, Latency, and Throughput ***

Weakscaling:
- Variable number of processors with fixed problem size per processor
- Accomplish more work in the same time

Strongscaling:
- Increasing number of processors
- Same work in less size

Thoughput = (#tasks) / time	[actions per time]

Latency = time / task	[time]

Speedup = sequential_execution_time / parallel_execution_time_with_N_workers

*** Amdahl's Law ***

P = Portion of the program that's parallelizable
S = Speedup of the parallelized portion

Overlall Speedup = 1 / ( (1 - P) + (P / S) )

Running the program only makes sense if the program is highly parallelizable.

*** Measure Speedup ***

Efficiently: How well the additional resources are utilized

Efiiciency = Speedup / # processors

Some languages may utlized just-in-time compilation that compile the parts
of programs at runtime. This may effect the result.


******** LECTURE 5 : Designing Parallel Programs ********

4-stages of taking a problem and creating parallel program:
1. Partitionin
2. Communication
3. Agglomeration
4. Mapping

*** Partitionin ***

Break down the problem into discreate multiple tasks.
Decompose the problem into as many small tasks as possible.

- Domain (data) decomposition: Dividing the data associated with the problem
	(if possible equally sized partitions)
	- Block Decomposition
	- Cyclic Decomposition

- Functional decomposition: Begins by considering all the computational work

*** Communication ***

Coordinate task execution and share information:

- Point-to-point communication: producer - consumer
- Collective communication: broadcast - scatter - gather

Synchronous Blocking Communication:
- Tasks wait until entire communication is completed
- Cannot do other work while in progress

Asynchronous Nonblocking Communication:
- Tasks do not wait for communication to complete
- Can do other work while in progres

Also consider the 'overhead', 'latency', 'bandwidth'

*** Agglomeration ***

Combine tasks and replicate data/computaion to increase efficiency

Granularity = computation / communication

Fine-Grained Parallesim
- Large number of small tasks
- Advantage - good distribution of workload (load balancing)
- Disadvantage - low computatiom-to-communication ratio

Coarse-Grained Parallesim
- Small number of large tasks
- Advatage - High computation-to-communication ratio
- Disadvantage - Inefficient load balancing


A well designed parallel program should adapt to changes in
the number of processors. (flexibility)

*** Mapping ***

Specify where eeach taks will execute to minimize the total execution time.

(does not apply for single-core or automated-task-scheduling systems)
In desktop computer programs this is not a problem:
The OS handles scheduling thread to execute on specific processor cores

This is a factor on:
- Distributed systems
- Specilalized hardware with multiple cores

Strategies:
- Place tasks that can run concurrently on the different processor
- Place tasks that communicate frequently on the same processor









